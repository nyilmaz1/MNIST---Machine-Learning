{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST classification:   \n",
    "The dataset provides 28x28 images of handwritten digits (1 per image) and the goal is to write an algorithm that detects which digit is written. this is a classification problem with 10 classes. We will build a network with 2 hidden layers between inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Outline the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Mean loss (training loss): 0.228. Validation loss: 0.108. Validation accuracy: 96.90%\n",
      "Epoch 2. Mean loss (training loss): 0.086. Validation loss: 0.091. Validation accuracy: 97.30%\n",
      "Epoch 3. Mean loss (training loss): 0.055. Validation loss: 0.081. Validation accuracy: 97.42%\n",
      "Epoch 4. Mean loss (training loss): 0.038. Validation loss: 0.069. Validation accuracy: 98.14%\n",
      "Epoch 5. Mean loss (training loss): 0.029. Validation loss: 0.068. Validation accuracy: 98.16%\n",
      "Epoch 6. Mean loss (training loss): 0.024. Validation loss: 0.074. Validation accuracy: 97.88%\n",
      "End of training.\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 400\n",
    "#reset any variables left in memory from previous runs.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#declare placeholders where the data will be fed into.\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "#weights and biases fro the first linear combination between the inputs and the first hidden layer.\n",
    "#use get_variable: the default TensorFlow initializer which is Xavier\n",
    "weights_1 = tf.get_variable('weights_1', [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable('biases_1', [hidden_layer_size])\n",
    "#operation between the inputs and the first hidden layer:\n",
    "#we have chosen ReLu as our activation function. try playing with other non-linearities\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "#tf.nn: is a module that contains neural network(nn) support. among other things, it contains the most commonly used activation functions. \n",
    "#other activation functions:\n",
    "#tf.nn.sigmoid\n",
    "#tf.nn.tanh\n",
    "#tf.nn.relu\n",
    "#tf.nn.softmax\n",
    "\n",
    "#weights and biases for the second linear combination. this is between the first and second hidden layer\n",
    "weights_2 = tf.get_variable('weights_2', [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable('biases_2', [hidden_layer_size])\n",
    "#operation between the first and the second hidden layers:\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) +biases_2)\n",
    "\n",
    "#weights and biases for the final linear combination: that's between the second hidden layer and the output layer\n",
    "weights_3 = tf.get_variable('weights_3', [hidden_layer_size, output_size])\n",
    "biases_3 = tf.get_variable('biases_3', [output_size])\n",
    "\n",
    "\n",
    "#operation between the second hidden layer and the output layer.\n",
    "#in this last operation we don't use an activation function.\n",
    "#because we will trick to include it directly in the loss function. this works for softmax and sigmoid activation functions\n",
    "outputs = tf.matmul(outputs_2, weights_3) + biases_3 #this is only calculating the linear combination (no activation function)\n",
    "\n",
    "\n",
    "\n",
    "#now we're going to calculate the loss function for every output/target layer\n",
    "#the function used is the same as applying softmax to the last layer and then calculating cross entropy error (an alternative to mean square error)\n",
    "#this function combines them in a clever way which makes it both faster and more numerically stable\n",
    "#logits here means: unscaled probabilities (the outputs before they are scaled by the softmax)\n",
    "#the labels are the targets\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels = targets)\n",
    "#get the average loss:\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "#define the optimization step. using adaptive optimizers such as adam in tensorflow, instead of gradient descent:\n",
    "optimize = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(mean_loss)\n",
    "\n",
    "#get a 0 or 1 for every input in the batch indicating whether it output the correct answer out of the 10:\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "\n",
    "#tf.argmax: we are only interested in the highest value in a given output row. this function returns the index of the column with the largest value\n",
    "# if the indeces of the outputs are matching with the indeces of targets, the model has predicted equally otherwise it hasn't.\n",
    "#tf.equal returns a boolean 1 if they match 0 if they dont\n",
    "# the number 1 in the tf.argmax function refers to axis=1 which refers to the column axes.\n",
    "#so, out_equals_target is a vector with one column with 1s and 0s. 1 for matches and 0 for mismatches\n",
    "\n",
    "#get the average accuracy of the outputs:\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))#tensorflow method of finding the mean\n",
    "#accuracy is the mean of the vector out_equals_target\n",
    "#we change it to float, because it's a boolean vector.\n",
    "#tf.cast: casts a tensor to a new type\n",
    "\n",
    "\n",
    "#declare the session variable:\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#initialize the variables:\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "\n",
    "#now we need the batch:\n",
    "batch_size=100\n",
    "\n",
    "#calculate the number of batches per epoch for the training set:\n",
    "batches_number = mnist.train._num_examples // batch_size\n",
    "\n",
    "#basic early stopping. set a maximum number of epochs:\n",
    "max_epochs = 15\n",
    "\n",
    "#keep trach of the validation loss of the previous epoch\n",
    "#if the validation loss becomes increasing, we want to trigger early stopping\n",
    "#we initially set it at some arbitrarily high number to make sure we don't trigger it at the first epoch\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "\n",
    "#learning:\n",
    "#create a loop for the epochs. epoch_encounter is a variable which automatically starts from 0.\n",
    "\n",
    "for epoch_counter in range(max_epochs):\n",
    "    curr_epoch_loss = 0.\n",
    "    for batch_counter in range(batches_number):\n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "        _, batch_loss = sess.run([optimize, mean_loss],\n",
    "            feed_dict = {inputs: input_batch, targets: target_batch})\n",
    "        curr_epoch_loss += batch_loss\n",
    "    curr_epoch_loss /= batches_number\n",
    "    input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    validation_loss, validation_accuracy = sess.run([mean_loss, accuracy],\n",
    "        feed_dict = {inputs: input_batch, targets: target_batch})\n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Mean loss (training loss): '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "    prev_validation_loss = validation_loss\n",
    "print('End of training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9817]\n",
      "test accuracy:98.17%\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "test_accuracy = sess.run([accuracy],\n",
    "    feed_dict ={inputs: input_batch, targets: target_batch})\n",
    "print(test_accuracy)\n",
    "\n",
    "test_accuracy_percent = test_accuracy[0] *100\n",
    "\n",
    "print('test accuracy:' + '{0:.2f}'.format(test_accuracy_percent) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hidden layer size = 50: test accuracy: 96.84\n",
    "#hidden layer size = 200: test accuracy: 97.69 (longer time)\n",
    "#hidden layer size = 400: test accuracy: 97.92 (takes much longer)\n",
    "#hidden layer size = 200 + one more layer: test accur: 96.86\n",
    "#hidden layer size = 200 + 5 hidden layers in  total: test accur: 96.91\n",
    "#hidden layer size = 400 + 5 hidden layers in total: test accur: 97.14 (long)\n",
    "#hidden layer size = 200 2 layers in total: with sigmoid in the last layer output (output_2): test accur: 97.58\n",
    "#hidden layer size = 400 2 layers in total: with sigmoid \" : test accur: 97.92\n",
    "#hidden layer size = 400 2 layers in total: with softmax \" : test accur: 70.21\n",
    "#hidden layer size = 400 2 layers in total: with tanh \" : test accur: 97.56\n",
    "#hidden layer size = 400 2 layers in total: with softmax both outputs : test accur: 93.63 \n",
    "#started low validation accuracy; took longer; more epochs; \n",
    "#hidden layer size = 400 2 layers in total: with sigmoid both outputs : test accur: 97.82\n",
    "#hidden layer size = 400 2 layers in total: batch size = 1000: test accur: 97.73\n",
    "#hidden layer size = 400 2 layers in total: batch size = 1: test accur: (takes extra extra long)\n",
    "#hidden layer size = 400 2 layers in total: learning rate = 0.0001: test accur: (takes longer; more epoch) 97.88\n",
    "#hidden layer size = 400 2 layers in total: learning rate = 0.02: test accur: 94.46 (takes less time; less epochs)\n",
    "#hidden layer size = 400 2 layers in total: learning rate = 0.01: test accur: 96.49\n",
    "#hidden layer size = 600 2 layers in total: learning rate = 0.001: test accur: 97.81\n",
    "#hidden layer size = 400 2 layers in total: learning rate = 0.001: test accur: 98.02\n",
    "#hidden layer size = 400 2 layers in total: batch size = 500: test accur: 97.94\n",
    "#hidden layer size = 400 2 layers in total: learning rate = 0.001: test accur: 98.17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
